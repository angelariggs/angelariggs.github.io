---
layout: post
title:  "Episode #1: Going On an Adventure! The First Step"
date:   2022-03-31
permalink: /articles/metrics-episode-1
comments: true
tags: guest-post metrics series-managing-metrics
---

Earlier this year I had a fantastic guest post from Mojtaba Hosseini, a Director of Engineering at Zapier, talking about the [12 pitfalls of engineering metrics]({% post_url 2022-01-18-pitfalls-of-engineering-metrics %}). As Mojtaba is guiding his current company through their journey of becoming a more metric-driven engineering organization, heâ€™s reflecting back on his previous experiences around metrics in the hope that those lessons learned can help inform the journey at Zapier (and hopefully help you, too ğŸ˜„)! So grab your favorite snack and settle in for the first flashback episode in Mojtabaâ€™s metrics series ğŸ¿

## The One Where Questions Lead to a Shocking Metric

When I first joined my previous engineering organization, we had no metrics at all. None! I had recently read [the book Accelerate](https://www.amazon.ca/Accelerate-Software-Performing-Technology-Organizations/dp/1942788339/) and so the 4 DORA metrics were very top of mind. I figured we would start with 2 of the easier ones to measure (focused on velocity), leaving the more difficult ones (focused on quality) for later:

- **Lead Time:** Time between an improvement being done (or scoped) and it being in production
- **Deployment Frequency:** How often such improvements make it to production

Easy enough! Right?

## Chasing Lead Time: Part 1

So I started to ask when the software engineering team deployed into production. The conversation went something like this _(dramatized for effect)_:

**Me:** â€œSo how often do we deploy?â€  
**Dev Team:** â€œWe release our software every 2 weeks at the end of each sprint.â€  
**Me:** â€œOh thatâ€™s great! So what happens when you release every 2 weeks?â€  
**Dev Team:** â€œWe start another sprint!â€  
**Me:** â€œNo. I mean what happens to the release of the completed sprint?â€  
**Dev Team:** â€œNot sure. Talk to our Operation team.â€  

ğŸ˜¬

## Interlude: Some Context  
It turns out I had joined an organization that was very early in its DevOps journey. There was a (software) Dev team with its own management chain, and an Ops Team with its own parallel management chain going up to the General Manager (VP of the division). Well this will be fun!

## Chasing Lead Time: Part 2

So hereâ€™s how my conversation went with the Ops Team _(dramatized for effect)_:

**Me:** â€œSo how often do we deploy?â€  
**Ops Team:** â€œWell, we used to deploy releases as soon as they became available.â€  
**Me:** â€œUsed to?â€  
**Ops Team:** â€œWell, several months ago, the Dev Team released a version of the software that forced a major Ubuntu upgrade on our VMs that broke all our scripts. We are a single-tenant application and so have one VM per customer. This means having to patch many many VMs with the updated scripts and doing the major upgrade. Weâ€™ve been working on a large upgrade script to do this for some time now.â€  
**Me:** How long?â€  
**Ops Team:** â€œCouple of months. We do the script development on the side. Our full time job is taken up by regular day to day ops stuff (security updates, new VMs, new customer installations, etc) as well as supporting our field operations team.â€  
**Me:** â€œSo when was the version currently in production released?â€  
**Ops Team:** â€œOhâ€¦ about a couple of months ago.â€  

ğŸ˜¬

## Lead Time Shocker

So it turned out the Lead Time for this engineering organization was 75 days and counting! The Dev Team happily released software every 2 weeks, but the Ops Team hadnâ€™t deployed anything to production for 75 days and counting!
This also meant that deployment frequency was essentially 0!
(For comparison, organizations in â€˜eliteâ€™ category deploy 10s or 100s of times per day and have lead times measured in minutes and hours.)

## On the Next Episode

Stay tuned for the next episode where we showed the Lead Time â€œgraphâ€ to the executives and hilarity ensued. _(Narrator: it did not!)_

## Lessons Learned

There were a few key takeaways for me here:
- Metrics are a means to an end. They help us ask questions.
- Measuring something, almost anything, is better than not measuring anything. That is - taking a baby step, even if itâ€™s not perfect, is a good way to start. We didnâ€™t try to capture all 4 DORA metrics, or to automate their collection. We started with baby steps.
- Metrics can dramatize and accentuate what is already known. Both the Dev and Ops teams knew of the problem but to say â€œour deployment frequency is 0â€ and â€œlead time is at 75 days and counting!â€ really crystalised the problem. 
- (the right) Metrics can help showcase outcomes that come together with the help of multiple teams. â€œDeployment frequencyâ€ captures many aspects of the velocity of the software development life cycle (from design to development, testing to CI/CD). Even if there are multiple people/teams that own pieces of the life cycle, this metric (and others) rally them around a set of numbers to improve together.

_If you're on the edge of your seat waiting to see how the executives reacted to the Lead Time metrics - don't worry! [Episode 2]({% post_url 2022-03-31-metrics-ep2 %}) of our Managing Metrics series has just dropped ğŸ’¥_
